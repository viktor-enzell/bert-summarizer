{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize articles and categories by averaging all BERT tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "bert_model_path = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2'\n",
    "bert_preprocessing_path = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
    "\n",
    "bert = hub.KerasLayer(bert_model_path, trainable=False, name='BERT');\n",
    "bert_preprocessing = hub.KerasLayer(bert_preprocessing_path, name='preprocessing');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using batch size = 100% of samples for preprocessing to work\n",
    "percent_train_data = 10\n",
    "num_training_samples = 1200 * percent_train_data\n",
    "\n",
    "train_data, _ = tfds.load(\n",
    "    name='ag_news_subset',\n",
    "    split=(f'train[:{percent_train_data}%]', 'test'),\n",
    "    shuffle_files=False,\n",
    "    as_supervised=True,\n",
    "    batch_size=num_training_samples\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='description')\n",
    "    encoder_inputs = bert_preprocessing(text_input)\n",
    "    outputs = bert(encoder_inputs)\n",
    "\n",
    "    # Retrieve the token embeddings for each token\n",
    "    net = outputs['sequence_output']\n",
    "    \n",
    "    # Build and compile the model\n",
    "    model = tf.keras.Model(text_input, net)\n",
    "    model.compile(\n",
    "        optimizer='Adam',\n",
    "        loss='SparseCategoricalCrossentropy',\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/viktorenzell/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def mean_pooling(predictions, sentences):\n",
    "    # Average the token embeddings, discarding spacial tokens ([CLS], [SEP], [PAD])\n",
    "    all_input_word_ids = bert_preprocessing(sentences)['input_word_ids'].numpy()\n",
    "    all_mean_pools = []\n",
    "\n",
    "    for i, input_word_ids in enumerate(all_input_word_ids):\n",
    "        # Count WordPiece tokens by:\n",
    "        # discarding [PAD] with np.non_zero()\n",
    "        # discarding [CLS] and [SEP] with -2\n",
    "        num_input_tokens = np.count_nonzero(input_word_ids) - 2\n",
    "        embeddings = predictions[i]\n",
    "        input_token_embeddings = embeddings[1:num_input_tokens + 1]\n",
    "        mean_pool = np.average(input_token_embeddings, axis=0)\n",
    "        all_mean_pools.append(mean_pool)\n",
    "    \n",
    "    return np.array(all_mean_pools)\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    " \n",
    "def get_synonym_strings(word_lists):\n",
    "    all_synonym_strings = []\n",
    "    for word_list in word_lists:\n",
    "        syn = []\n",
    "        syn_str = ''\n",
    "        for word in word_list:\n",
    "            for synset in wordnet.synsets(word):\n",
    "                for lemma in synset.lemmas():\n",
    "                    parsed_lemma = lemma.name().replace('_', ' ')\n",
    "                    if parsed_lemma not in syn:\n",
    "                        syn.append(parsed_lemma)\n",
    "                        syn_str += parsed_lemma + '. '\n",
    "        \n",
    "        all_synonym_strings.append(syn)\n",
    "    return all_synonym_strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize, average and save articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(train_data)\n",
    "sentences = np.array(list(train_data))[:,0,:][0]\n",
    "\n",
    "avg_article_embeddings = mean_pooling(predictions, sentences)\n",
    "\n",
    "labels = np.concatenate([[label] for _, label in train_data], axis=1)\n",
    "article_data = np.append(avg_article_embeddings, labels.T, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df = pd.DataFrame(article_data)\n",
    "article_df.to_csv(f'../../data/articles_avg_token.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize, average and save categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories as category names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_labels = ['Politics', 'Sports', 'Business', 'Science and Technology']\n",
    "category_embeddings = model.predict(category_labels)\n",
    "avg_category_embeddings = mean_pooling(category_embeddings, category_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_df = pd.DataFrame(avg_category_embeddings)\n",
    "category_df.to_csv('../../data/categories_avg_token.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories as vectors with synonmys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['politics', 'political relation', 'political science', 'government', 'political sympathies']\n",
      "\n",
      "['sport', 'athletics', 'summercater', 'sportsman', 'sportswoman', 'mutant', 'mutation', 'variation', 'fun', 'play', 'feature', 'boast', 'frolic', 'lark', 'rollick', 'skylark', 'disport', 'cavort', 'gambol', 'frisk', 'romp', 'run around', 'lark about']\n",
      "\n",
      "['business', 'concern', 'business concern', 'business organization', 'business organisation', 'commercial enterprise', 'business enterprise', 'occupation', 'job', 'line of work', 'line', 'business sector', 'clientele', 'patronage', 'stage business', 'byplay']\n",
      "\n",
      "['science', 'scientific discipline', 'skill', 'technology', 'engineering', 'engineering science', 'applied science']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "synonym_strings = get_synonym_strings([['Politics'], ['Sports'], ['Business'], ['Science', 'Technology']])\n",
    "\n",
    "for s in synonym_strings:\n",
    "    print(s)\n",
    "    print()\n",
    "\n",
    "avg_category_syn_embeddings = []\n",
    "for category_synonyms in synonym_strings:\n",
    "    mean_poolings = mean_pooling(model.predict(category_synonyms), category_synonyms)\n",
    "    avg_category_syn_embeddings.append(np.average(mean_poolings, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_syn_df = pd.DataFrame(avg_category_syn_embeddings)\n",
    "category_syn_df.to_csv('../../data/categories_avg_synonym.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "05027191b138f41f3a3ce51d90f6c7168b5678add389de47d7efac856dc8b51a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('hle': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
