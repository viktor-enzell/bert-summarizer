{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize articles and categories by using BERT CLS tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "bert_model_path = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2'\n",
    "bert_preprocessing_path = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
    "\n",
    "bert = hub.KerasLayer(bert_model_path, trainable = False, name = 'BERT');\n",
    "bert_preprocessing = hub.KerasLayer(bert_preprocessing_path, name='preprocessing');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_train_data = 10\n",
    "\n",
    "train_data, _ = tfds.load(\n",
    "    name='ag_news_subset',\n",
    "    split=(f'train[:{percent_train_data}%]', 'test'),\n",
    "    shuffle_files=False,\n",
    "    as_supervised=True,\n",
    "    batch_size=1\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='description')\n",
    "    encoder_inputs = bert_preprocessing(text_input)\n",
    "    outputs = bert(encoder_inputs)\n",
    "\n",
    "    # Only retrieve the outputs from the corresponding [CLS] token\n",
    "    net = outputs['pooled_output']\n",
    "    \n",
    "    # Build and compile the model\n",
    "    model = tf.keras.Model(text_input, net)\n",
    "    model.compile(\n",
    "        optimizer='Adam',\n",
    "        loss='SparseCategoricalCrossentropy',\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve CLS vectors and save articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_embeddings = model.predict(train_data)\n",
    "labels = np.concatenate([[label] for _, label in train_data], axis=1)\n",
    "article_data = np.append(article_embeddings, labels.T, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df = pd.DataFrame(article_data)\n",
    "article_df.to_csv(f'../../data/articles_cls_token.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve CLS vectors and save categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_embeddings = model.predict(['World', 'Sports', 'Business', 'Science and Technology'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_df = pd.DataFrame(category_embeddings)\n",
    "category_df.to_csv('../../data/categories_cls_token.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "05027191b138f41f3a3ce51d90f6c7168b5678add389de47d7efac856dc8b51a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('hle': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
