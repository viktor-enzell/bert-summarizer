{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run imports and set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_path = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2'\n",
    "bert_preprocessing_path = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
    "\n",
    "model_path = '../../models/bert'\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 3\n",
    "percent_train_data = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "Import the dataset from Tensorflow Hub and split it into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = tfds.load(\n",
    "    name='ag_news_subset',\n",
    "    split=(f'train[:{percent_train_data}%]', 'test'),\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    batch_size=batch_size\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limiting the amount of data by number of batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 16 batches and 512 samples\n"
     ]
    }
   ],
   "source": [
    "num_batches = 32\n",
    "train_data = train_data.take(num_batches)\n",
    "\n",
    "print(f'Using {num_batches} batches and {num_batches * batch_size} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import BERT model and preprocessing handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-19 12:53:42.340587: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    }
   ],
   "source": [
    "bert_preprocessing = hub.KerasLayer(bert_preprocessing_path, name='preprocessing');\n",
    "bert = hub.KerasLayer(bert_model_path, trainable=True, name = 'BERT');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "We create a function to define and compile the NN with the pretrained BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "description (InputLayer)        [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "preprocessing (KerasLayer)      {'input_type_ids': ( 0           description[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "BERT (KerasLayer)               {'encoder_outputs':  4385921     preprocessing[9][0]              \n",
      "                                                                 preprocessing[9][1]              \n",
      "                                                                 preprocessing[9][2]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 4)            516         BERT[9][3]                       \n",
      "==================================================================================================\n",
      "Total params: 4,386,437\n",
      "Trainable params: 4,386,436\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model():\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='description')\n",
    "    encoder_inputs = bert_preprocessing(text_input)\n",
    "    outputs = bert(encoder_inputs)\n",
    "\n",
    "    # Only retrieve the outputs from the corresponding [CLS] token\n",
    "    net = outputs['pooled_output']\n",
    "\n",
    "    # Additional layer for classification\n",
    "    net = tf.keras.layers.Dense(4, activation='softmax')(net)\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = tf.keras.Model(text_input, net)\n",
    "    model.compile(\n",
    "        optimizer='Adam',\n",
    "        loss='SparseCategoricalCrossentropy',\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "Now that the model is compiled, we can train on our data. We use early stopping to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 4635), started 0:30:55 ago. (Use '!kill 4635' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-25b97ad33cbee01e\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-25b97ad33cbee01e\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-19 13:22:29.297383: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2021-12-19 13:22:29.297400: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2021-12-19 13:22:29.298196: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      " 1/16 [>.............................] - ETA: 36s - loss: 1.5996 - sparse_categorical_accuracy: 0.3125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-19 13:22:32.084803: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2021-12-19 13:22:32.084820: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2/16 [==>...........................] - ETA: 6s - loss: 1.3117 - sparse_categorical_accuracy: 0.3906 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-19 13:22:32.587948: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2021-12-19 13:22:32.595945: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2021-12-19 13:22:32.602559: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: logs/fit/20211219-132229/train/plugins/profile/2021_12_19_13_22_32\n",
      "\n",
      "2021-12-19 13:22:32.608010: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to logs/fit/20211219-132229/train/plugins/profile/2021_12_19_13_22_32/Viktors-MacBook-Pro.local.trace.json.gz\n",
      "2021-12-19 13:22:32.622640: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: logs/fit/20211219-132229/train/plugins/profile/2021_12_19_13_22_32\n",
      "\n",
      "2021-12-19 13:22:32.622916: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20211219-132229/train/plugins/profile/2021_12_19_13_22_32/Viktors-MacBook-Pro.local.memory_profile.json.gz\n",
      "2021-12-19 13:22:32.625302: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: logs/fit/20211219-132229/train/plugins/profile/2021_12_19_13_22_32\n",
      "Dumped tool data for xplane.pb to logs/fit/20211219-132229/train/plugins/profile/2021_12_19_13_22_32/Viktors-MacBook-Pro.local.xplane.pb\n",
      "Dumped tool data for overview_page.pb to logs/fit/20211219-132229/train/plugins/profile/2021_12_19_13_22_32/Viktors-MacBook-Pro.local.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to logs/fit/20211219-132229/train/plugins/profile/2021_12_19_13_22_32/Viktors-MacBook-Pro.local.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to logs/fit/20211219-132229/train/plugins/profile/2021_12_19_13_22_32/Viktors-MacBook-Pro.local.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to logs/fit/20211219-132229/train/plugins/profile/2021_12_19_13_22_32/Viktors-MacBook-Pro.local.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 25s 1s/step - loss: 0.6465 - sparse_categorical_accuracy: 0.7969 - val_loss: 0.9433 - val_sparse_categorical_accuracy: 0.7467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-19 13:22:54.650798: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3\n",
      "16/16 [==============================] - 21s 1s/step - loss: 0.3197 - sparse_categorical_accuracy: 0.9160 - val_loss: 0.7524 - val_sparse_categorical_accuracy: 0.7721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-19 13:23:16.521193: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3\n",
      "16/16 [==============================] - 21s 1s/step - loss: 0.2058 - sparse_categorical_accuracy: 0.9395 - val_loss: 1.0238 - val_sparse_categorical_accuracy: 0.7554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-19 13:23:37.802511: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "earlystopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_sparse_categorical_accuracy', \n",
    "    patience=3, \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir='logs/fit/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S'), \n",
    "    histogram_freq=1, \n",
    "    update_freq='batch'\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    x=train_data,\n",
    "    validation_data=test_data,\n",
    "    epochs=num_epochs,\n",
    "    callbacks=[earlystopping_callback, tensorboard_callback],\n",
    "    batch_size=batch_size,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.5322572e-03, 9.9693835e-01, 3.6762521e-04, 1.6190905e-04]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "model.predict(['i play a lot of fotball, sports is nice, who scored most goals'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238/238 [==============================] - 18s 75ms/step - loss: 1.1623 - sparse_categorical_accuracy: 0.5491\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1623013019561768, 0.5490789413452148]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-17 16:13:40.545074: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 165). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/bert/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/bert/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot comparison between fine-tuning with different amounts of data and dataless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_points = [128, 256, 384, 512, 640, 768, 896, 1024]\n",
    "fine_tuning = [63.6, 73.8, 77.0, 75.5, 78.6, 82.2, 82.3, 82.5]\n",
    "dataless = [77.6 for _ in range(8)]\n",
    "\n",
    "plt.plot(x_points, fine_tuning, color='black', marker='o' , label='Fine-tuning')\n",
    "plt.plot(x_points, dataless, color='black', linestyle='dashed', label='Dataless')\n",
    "\n",
    "plt.xlabel('Number of training samples')\n",
    "plt.ylabel('Test accuracy (%)')\n",
    "plt.xticks(x_points, x_points)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
