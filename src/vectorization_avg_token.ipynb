{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize articles and categories by averaging all BERT tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "bert_model_path = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2'\n",
    "bert_preprocessing_path = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
    "\n",
    "bert = hub.KerasLayer(bert_model_path, trainable = False, name = 'BERT');\n",
    "bert_preprocessing = hub.KerasLayer(bert_preprocessing_path, name='preprocessing');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_train_data = 10\n",
    "\n",
    "train_data, _ = tfds.load(\n",
    "    name='ag_news_subset',\n",
    "    split=(f'train[:{percent_train_data}%]', 'test'),\n",
    "    shuffle_files=False,\n",
    "    as_supervised=True,\n",
    "    batch_size=1\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='description')\n",
    "    encoder_inputs = bert_preprocessing(text_input)\n",
    "    outputs = bert(encoder_inputs)\n",
    "\n",
    "    # Retrieve the token embeddings for each token\n",
    "    net = outputs['sequence_output']\n",
    "    \n",
    "    # Build and compile the model\n",
    "    model = tf.keras.Model(text_input, net)\n",
    "    model.compile(\n",
    "        optimizer='Adam',\n",
    "        loss='SparseCategoricalCrossentropy',\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize, average and save articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_embeddings = model.predict(train_data)\n",
    "avg_article_embeddings = np.array([np.average(embs, axis=0) for embs in article_embeddings])\n",
    "labels = np.concatenate([[label] for _, label in train_data], axis=1)\n",
    "article_data = np.append(avg_article_embeddings, labels.T, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df = pd.DataFrame(article_data)\n",
    "article_df.to_csv(f'../data/articles_avg_token.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize, average and save categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories as category names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_embeddings = model.predict(['World', 'Sports', 'Business', 'Science and Technology'])\n",
    "avg_category_embeddings = np.array([np.average(embs, axis=0) for embs in category_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_df = pd.DataFrame(avg_category_embeddings)\n",
    "category_df.to_csv('../data/categories_avg_token.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories as vectors with synonmys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/viktorenzell/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    " \n",
    "def get_synonym_strings(words):\n",
    "    all_synonym_strings = []\n",
    "    for word in words:\n",
    "        syn = ''\n",
    "        for synset in wordnet.synsets(word):\n",
    "            for lemma in synset.lemmas():\n",
    "                syn += lemma.name() + '. '\n",
    "        all_synonym_strings.append(syn)\n",
    "    return all_synonym_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonym_strings = get_synonym_strings(['World', 'Sports', 'Business', 'Technology'])\n",
    "category_syn_embeddings = model.predict(synonym_strings)\n",
    "avg_category_syn_embeddings = np.array([np.average(embs, axis=0) for embs in category_syn_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_syn_df = pd.DataFrame(avg_category_syn_embeddings)\n",
    "category_syn_df.to_csv('../data/categories_avg_synonym.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "05027191b138f41f3a3ce51d90f6c7168b5678add389de47d7efac856dc8b51a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('hle': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
